{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Pipelining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with ColumnTransformer on a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "data1 = np.column_stack((np.random.uniform(-2,1,5), np.random.randint(0,10,5)))\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "print(StandardScaler().fit_transform(data1))\n",
    "print()\n",
    "print(MinMaxScaler().fit_transform(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's perform Z-normalization on the zeroth column and MinMaxScaling on the first\n",
    "c0 = StandardScaler().fit_transform(data1[:,0:1]) #data1[:,0:1] vs data1[:,0] ?\n",
    "c1 = MinMaxScaler().fit_transform(data1[:,1:])\n",
    "\n",
    "print(np.column_stack((c0,c1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([ ('znorm', StandardScaler(), [0]), ('minmax', MinMaxScaler(), [1]) ])\n",
    "\n",
    "data1ct =ct.fit_transform(data1)\n",
    "\n",
    "print(data1ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What did we do?**  \n",
    "We specified the name and type of the transformers and the columns that they should affect. It is your turn, let's do something similar below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "data2 = [[0.5, 1.2, -2.3, -0.7, 3.7],\n",
    "         ['a', 'b', None, 'b', 'c']]\n",
    "\n",
    "y = [0, 1, 0, 0, 1]\n",
    "\n",
    "# Put these into a pandas dataframe. Note that the rows of data2 should be the columns of the data!\n",
    "\n",
    "df = pd.DataFrame(data2)\n",
    "display(df)\n",
    "df = df.T\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform z-normalization on the first column\n",
    "# Perform most frequent imputation to the second column followed by one-hot encoding. \n",
    "# Hint: ColumnTransformers can be used in a Pipeline and Pipelines can be used in ColumnTransformers \n",
    "\n",
    "#SimpleImputer + OneHotEncoder Pipeline\n",
    "pp_pipe = Pipeline([('imp',SimpleImputer(strategy = \"most_frequent\")), \n",
    "                    ('ohe', OneHotEncoder(handle_unknown = \"ignore\"))])\n",
    "display(pp_pipe.fit_transform(df.iloc[:,1:]).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ct = ColumnTransformer([ ('znorm', StandardScaler(), [0]), ('minmax', MinMaxScaler(),[1]) ])\n",
    "ct = ColumnTransformer([ ('znorm', StandardScaler(), [0]), ('pipe', pp_pipe, [1]) ])\n",
    "\n",
    "display(ct.fit_transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a decision tree classifier \n",
    "#dt = DecisionTreeClassifier().fit(ct.fit_transform(df))\n",
    "\n",
    "#something.fit(df, y)\n",
    "main_pipe = Pipeline([('pp_all',ct), ('dt_classifier', DecisionTreeClassifier())])\n",
    "main_pipe.fit(df, y)\n",
    "print(main_pipe.score(df,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "\n",
    "cv = StratifiedKFold(n_splits = 2 ,shuffle=True)\n",
    "\n",
    "params_to_search = {'dt_classifier__max_depth':[2,3,4,None]}\n",
    "\n",
    "gs = GridSearchCV(estimator = main_pipe, param_grid = params_to_search, cv = cv)\n",
    "gs.fit(df,y)\n",
    "gs.score(df,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detour: Python Slice Feature  \n",
    "`slice(start,stop,step)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.arange(10)\n",
    "print(tmp)\n",
    "print(tmp[slice(3)], tmp[:3])\n",
    "print(tmp[slice(3,7)], tmp[3:7])\n",
    "print(tmp[slice(1,7,2)], tmp[1:7:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we had more columns that we do not want to touch or where it is impractical to list all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress = True)\n",
    "\n",
    "data3 = np.random.random((15,10))\n",
    "\n",
    "pp1 = StandardScaler()\n",
    "pp2 = MinMaxScaler((-1,0))\n",
    "\n",
    "ct_more1 = ColumnTransformer([('znorm', pp1, [0,1]), ('minmax',pp2, slice(3,7))])\n",
    "\n",
    "data3ct1 = ct_more1.fit_transform(data3)\n",
    "print(data3ct1.shape)\n",
    "print(data3ct1.mean(axis=0))\n",
    "print(data3ct1.std(axis=0))\n",
    "print(data3ct1.min(axis=0))\n",
    "print(data3ct1.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do with the remaining columns?\n",
    "* Drop them (default behavior)\n",
    "* Pass them as is\n",
    "* Apply another transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_more2 = ColumnTransformer([('znorm', pp1, [0,1]), ('minmax',pp2, slice(3,7))], remainder='passthrough')\n",
    "\n",
    "data3ct2 = ct_more2.fit_transform(data3)\n",
    "print(data3ct2.shape)\n",
    "print(data3ct2.mean(axis=0))\n",
    "print(data3ct2.std(axis=0))\n",
    "print(data3ct2.min(axis=0))\n",
    "print(data3ct2.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_more3 = ColumnTransformer([('znorm', pp1, [0,1]), ('minmax',pp2, slice(3,7))], remainder=MinMaxScaler((1,2)))\n",
    "\n",
    "data3ct3 = ct_more3.fit_transform(data3)\n",
    "print(data3ct3.shape)\n",
    "print(data3ct3.mean(axis=0))\n",
    "print(data3ct3.std(axis=0))\n",
    "print(data3ct3.min(axis=0))\n",
    "print(data3ct3.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_more4 = ColumnTransformer([('znorm', pp1, [0,1]), ('dropped','drop', slice(3,7))], remainder=MinMaxScaler((1,2)))\n",
    "\n",
    "data3ct4 = ct_more4.fit_transform(data3)\n",
    "print(data3ct4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping the ordering\n",
    "ct_more5 = ColumnTransformer([('znorm', pp1, [0,1]),  \n",
    "                              ('keep','passthrough',[2]), \n",
    "                              ('minmax',pp2, slice(3,6))], \n",
    "                               remainder=MinMaxScaler((1,2)))\n",
    "\n",
    "data3ct5 = ct_more5.fit_transform(data3)\n",
    "print(data3ct5.shape)\n",
    "print(data3ct5.mean(axis=0))\n",
    "print(data3ct5.std(axis=0))\n",
    "print(data3ct5.min(axis=0))\n",
    "print(data3ct5.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping the ordering\n",
    "ct_more5 = ColumnTransformer([('znorm', pp1, [0,1]),  \n",
    "                              ('keep','passthrough',[9]),\n",
    "                              ('minmax',pp2, slice(3,6)),\n",
    "                              ], \n",
    "                               remainder=MinMaxScaler((1,2)))\n",
    "                              \n",
    "\n",
    "data3ct5 = ct_more5.fit_transform(data3)\n",
    "print(data3ct5.shape)\n",
    "print(data3ct5.mean(axis=0))\n",
    "print(data3ct5.std(axis=0))\n",
    "print(data3ct5.min(axis=0))\n",
    "print(data3ct5.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input is a pandas DataFrame, we can use the column names (in the first level) as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data2).T\n",
    "df = df.rename({0:'numbers',1:'categories'},axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline(steps = [('imp',SimpleImputer(strategy='most_frequent')),('enc',OneHotEncoder())])\n",
    "ct_nodf = ColumnTransformer([('znorm',StandardScaler(),[0]),('cat', cat_pipe,[1])])\n",
    "ct_nodf.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline(steps = [('imp',SimpleImputer(strategy='most_frequent')),('enc',OneHotEncoder())])\n",
    "ct_df = ColumnTransformer([('znorm',StandardScaler(),['numbers']),('cat', cat_pipe,['categories'])])\n",
    "ct_df.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a more complicated example but still with a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need an internet connection\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "\n",
    "# If the above doesn't work (loads it but with different column names)\n",
    "#from catboost.datasets import titanic\n",
    "#X, y = titanic()\n",
    "\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features:\n",
    "* sibsp: Number of Siblings/Spouses Aboard\n",
    "* parch: Number of Parents/Children Aboard\n",
    "* survival - Survival (0 = No; 1 = Yes)\n",
    "* pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "* ticket - Ticket Number\n",
    "* fare - Passenger Fare\n",
    "* cabin - Cabin\n",
    "* embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "* boat - Lifeboat (if survived)\n",
    "* body - Body number (if did not survive and body was recovered)\n",
    "* home.dest - Home destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe(include=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe(include='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we do?\n",
    "* Drop `name`, `ticket`, `boat`, `home.dest`, `cabin` and `body` (some of them e.g. cabin would be useful but we are making it easier) \n",
    "* `pclass`: Ordinal, may stay as is or may be treated as a categorical value\n",
    "* `sex`: To binary\n",
    "* `age` and `fare`: Standard scaler\n",
    "* `sibsp` and `parch`: log1p (count values) and perhaps maxabsolute scaler (lots of 0s) \n",
    "* `embarked`: One-Hot \n",
    "* impute if necessary\n",
    "\n",
    "These are the initial ideas, let's look at the data and see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['name','ticket','boat','home.dest','cabin','body'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['age','fare'],axis=1).boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(np.log1p(X['pclass'].max()-X['pclass']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(np.log1p(X['sibsp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(np.log1p(X['parch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((np.log1p(X['age'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X['fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist((np.log1p(X['fare'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def countplot(pd_series, ax = None):\n",
    "    counts = pd_series.value_counts()\n",
    "    if ax:\n",
    "        ax.bar(range(len(counts)),counts.values, color = mcolors.TABLEAU_COLORS)\n",
    "        plt.sca(ax)\n",
    "    else:\n",
    "        plt.bar(range(len(counts)),counts.values, color = mcolors.TABLEAU_COLORS)\n",
    "    plt.xticks(range(len(counts)), counts.keys())\n",
    "    return counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.countplot(X['embarked'])\n",
    "countplot(X['embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.countplot(X['sex'])\n",
    "countplot(X['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countplot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data counts\n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Look at survival statistics based on sex, age, fare, pclass etc. \n",
    "\n",
    "Check if they really did \"women and children first\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Survived based on gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_df = X.copy()\n",
    "ez_df['survived'] = y\n",
    "\n",
    "countplot(ez_df[ez_df['sex']=='male']['survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countplot(ez_df[ez_df['sex']=='female']['survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy = ((ez_df[ez_df['sex']=='female']['survived']=='1').sum()+(ez_df[ez_df['sex']=='male']['survived']=='0').sum())/1309\n",
    "print(baseline_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Women's survival rate is higher than men's. What about class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3,figsize=(12,6))\n",
    "for i in range(len(axs)):\n",
    "    countplot(ez_df[ez_df['pclass']==i+1]['survived'], ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Survival rate of passengers with a higher class ticket is more than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3,figsize=(12,6))\n",
    "for i in range(len(axs)):\n",
    "    countplot(ez_df[(ez_df['pclass']==i+1) & (ez_df['sex']=='female')]['survived'], ax=axs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3,figsize=(12,6))\n",
    "for i in range(len(axs)):\n",
    "    countplot(ez_df[(ez_df['pclass']==i+1) & (ez_df['sex']=='male')]['survived'], ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age_bins = {'0-6':0,'7-12':7,'13-18':13,'18-25':19,'26-40':26,'41-55':41,'55-80':56,'80':80}\n",
    "age_bins = {'0-12':0,'13-18':13,'18-25':19,'26-40':26,'41-80':41,'80':80}\n",
    "nums,edges = np.histogram(ez_df['age'],list(age_bins.values()))\n",
    "plt.bar(range(len(nums)),nums)\n",
    "plt.xticks(range(len(nums)),list(age_bins.keys())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group(x, edges):\n",
    "    #cs_edges = np.cumsum(edges)\n",
    "    for i,edge in enumerate(edges):\n",
    "        if x < edge:\n",
    "            return i-1\n",
    "    return len(edges)\n",
    "    \n",
    "ez_df['age group'] = [get_group(x,np.array(list(age_bins.values()),dtype='float')) for x in ez_df['age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([(ez_df[ez_df['age group']==group]['survived']=='1').sum()/len(ez_df[ez_df['age group']==group]['survived']) for group in range(len(age_bins)-1)])\n",
    "plt.xticks(range(len(age_bins)-1),list(age_bins.keys())[:-1])\n",
    "plt.title('Survival rate by age group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "500/1309"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The suggested steps (we can change it during the lecture!)\n",
    "\n",
    "**Lvl 1:**\n",
    "* sex: to binary, labelencoder is fine\n",
    "* embarked: missing value as the most frequent port (since we do not have many missing values)\n",
    "* embarked: to-one-hot\n",
    "\n",
    "**Lvl 2:**\n",
    "* overall iterative imputation with random forest regressor\n",
    "\n",
    "**Lvl 3:**\n",
    "* pclass: leave as is or min max scaler\n",
    "* sibsp and parch: log(1+x) (but we could easily remove parch)\n",
    "* age: standard scaling \n",
    "* add feature: age <= 12:0, age >12:1\n",
    "* fare: log(1+x) then standard scaling\n",
    "\n",
    "**Classifier:** LogisticRegression, Compare Random Forest and SVM\n",
    "\n",
    "Let's focus on the usage of pipelines and column transformers instead of the final accuracy for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=0.8, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test(clf, Xtrain, Xtest, ytrain, ytest, name = None, refit = True):\n",
    "    if refit:\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "    ytrainPred = clf.predict(Xtrain)\n",
    "    ytestPred = clf.predict(Xtest)\n",
    "    if name:\n",
    "        print(name)\n",
    "    else:\n",
    "        print(clf.steps[1][0])\n",
    "    print('Train:',accuracy_score(ytrain,ytrainPred))\n",
    "    print('Test:',accuracy_score(ytest,ytestPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed since LabelEncoder does not play nicely with pipelines\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class PipelineLabelEncoder(LabelEncoder):\n",
    "    # This is not entirely correct, we need a separate label encoder for each dimension!\n",
    "    def fit_transform(self, y, *args, **kwargs):\n",
    "        X = np.empty(y.shape)\n",
    "        df = False\n",
    "        if type(y) == pd.DataFrame:\n",
    "            df = True\n",
    "        for i in range(y.shape[1]):\n",
    "            if df:\n",
    "                X[:,i] = super().fit_transform(y.iloc[:,i])\n",
    "            else:\n",
    "                X[:,i] = super().fit_transform(y[:,i])\n",
    "        return X\n",
    "\n",
    "    def transform(self, y, *args, **kwargs):\n",
    "        X = np.empty(y.shape)\n",
    "        df = False\n",
    "        if type(y) == pd.DataFrame:\n",
    "            df = True\n",
    "        for i in range(y.shape[1]):\n",
    "            if df:\n",
    "                X[:,i] = super().transform(y.iloc[:,i])\n",
    "            else:\n",
    "                X[:,i] = super().transform(y[:,i])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline based on the EDA: Decision Tree CLassifier on just the sex and pclass\n",
    "# Need to convert sex to binary\n",
    "dt_ct = ColumnTransformer([('lenc',PipelineLabelEncoder(),['sex']),('pt','passthrough',['pclass'])],remainder='drop')\n",
    "dt_pipe = Pipeline([('preprocesser',dt_ct),('dt_classifier', DecisionTreeClassifier())])\n",
    "\n",
    "test(dt_pipe, Xtrain, Xtest, ytrain, ytest, name = 'Decision Tree', refit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(dt_pipe.named_steps['dt_classifier'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\"\"\"\n",
    "**Lvl 1:**\n",
    "* sex: to binary, labelencoder is fine\n",
    "* embarked: missing value as the most frequent port (since we do not have many missing values)\n",
    "* embarked: to-one-hot\n",
    "\"\"\"\n",
    "\n",
    "genderLenc = ('GenderLabelEncoder', PipelineLabelEncoder(), ['sex'])\n",
    "#genderLenc = ('GenderLabelEncoder', LabelEncoder(), ['sex']) uncomment to see\n",
    "\n",
    "embarkedInit = ('EmbarkedInit', \n",
    "                Pipeline([('EmbarkedImpute', SimpleImputer(strategy='most_frequent')),\n",
    "                          ('EmbarkedOneHot',OneHotEncoder(handle_unknown='ignore'))]),  ['embarked'])\n",
    "\n",
    "# passthrough changes the order. Furthermore we lose the pandas dataframe. After this, \n",
    "# we will need to keep track of the indices. This is a trade-off of scikit-learn where they \n",
    "# value numpy array compatibility over pandas dataframe compatibility but work is being done\n",
    "\n",
    "# order before: pclass, sex, age, sibsp, parch, fare, embarked\n",
    "# order after the below: sex, embarked x 3 (1 hot), pclass, age, sibsp, parch, fare\n",
    "firstLevelCT = ColumnTransformer([genderLenc, embarkedInit], remainder='passthrough')\n",
    "\n",
    "\"\"\"\n",
    "**Lvl 2:**\n",
    "* overall iterative imputation\n",
    "\"\"\"\n",
    "\n",
    "# order after the below: sex, embarked x 3 (1hot), pclass, age, sibsp, parch, fare, i.e., no change\n",
    "#allImputer = ('AllImpute', \n",
    "#               IterativeImputer(estimator=RandomForestRegressor(n_estimators=50), max_iter=10, tol=0.01), \n",
    "#               np.arange(9))\n",
    "#secondLevelCT = ColumnTransformer([allImputer])\n",
    "#secondLevel = Pipeline([('AllImpute',IterativeImputer(estimator=RandomForestRegressor(n_estimators=50), \n",
    "#                                                      max_iter=10, \n",
    "#                                                      tol=0.01))])\n",
    "\n",
    "secondLevel = IterativeImputer(estimator=RandomForestRegressor(n_estimators=50), \n",
    "                                                      max_iter=10, \n",
    "                                                      tol=0.01)\n",
    "\"\"\"\n",
    "**Lvl 3:**\n",
    "* pclass: leave as is or min max scaler\n",
    "* sibsp: log(1+x)\n",
    "* parch: drop\n",
    "* age: standard scaling\n",
    "* age to binary (<12) ?\n",
    "* fare: log(1+x) then standard scaling\n",
    "\"\"\"\n",
    "Log1pTransformer = FunctionTransformer(np.log1p, validate=True)\n",
    "sibsp = ('CountsLogT', Log1pTransformer, [6])\n",
    "parch = ('Drop','drop',[7])\n",
    "ageScaler = ('AgeScaler', StandardScaler(), [5])\n",
    "fare = ('Fare', Pipeline([('FareLT',Log1pTransformer), ('FareSc',StandardScaler())]), [8])\n",
    "\n",
    "# order after the below: sibsp, parch, age, fare, sex, embarked x 3 (1hot), pclass\n",
    "thirdLevelCT = ColumnTransformer([sibsp,parch,ageScaler,fare], remainder='passthrough')\n",
    "\n",
    "MainPipeLine = Pipeline ([('first', firstLevelCT),\n",
    "                          ('second', secondLevel),\n",
    "                          ('third', thirdLevelCT)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x1 = firstLevelCT.fit_transform(Xtrain)\n",
    "#print(x1.shape)\n",
    "#print(x1)\n",
    "x2 = secondLevel.fit_transform(x1)\n",
    "x3 = thirdLevelCT.fit_transform(x2)\n",
    "\n",
    "print(x1.shape,x2.shape,x3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x123 = MainPipeLine.fit_transform(Xtrain)\n",
    "print(x123.shape)\n",
    "print((x3-x123).sum(axis=1))\n",
    "\n",
    "# Discrepancy is due to RandomForestRegressor imputation since it introduces some randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save and load pipelines with the pickle module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(MainPipeLine,open('tmp_pp.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=pickle.load(open('tmp_pp.p','rb'))\n",
    "xtmp = tmp.transform(Xtrain)\n",
    "print((xtmp-x123).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LrPipeline  = Pipeline([('main', MainPipeLine), ('lr',  LogisticRegression())])\n",
    "SvmPipeline = Pipeline([('main', MainPipeLine), ('svm', SVC())])\n",
    "RfPipeline  = Pipeline([('main', MainPipeLine), ('rf',  RandomForestClassifier())])\n",
    "\n",
    "# Warning:: MainPipeLine is fitted each time and our iterative imputer is a bit expensive\n",
    "test(LrPipeline, Xtrain, Xtest, ytrain, ytest)\n",
    "test(SvmPipeline, Xtrain, Xtest, ytrain, ytest)\n",
    "test(RfPipeline, Xtrain, Xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster Alternative if the preprocessing steps are the same:\n",
    "XtrainPreProc = MainPipeLine.fit_transform(Xtrain)\n",
    "XtestPreProc = MainPipeLine.transform(Xtest)\n",
    "\n",
    "# Then call the LogisticRegression, SVC and RandomForestClassifier fit using XtrainPreProc\n",
    "dt2 = DecisionTreeClassifier()\n",
    "\n",
    "dt2.fit(XtrainPreProc, ytrain)\n",
    "ytrainDt = dt2.predict(XtrainPreProc)\n",
    "ytestDt = dt2.predict(XtestPreProc)\n",
    "print('Train:',accuracy_score(ytrain,ytrainDt))\n",
    "print('Test:',accuracy_score(ytest,ytestDt))\n",
    "\n",
    "#Worse than the first baseline??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about hyper parameter search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LrPipeline.steps[0][1].steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "# With grid search, does not change the result all that much\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "param_grid = {'main__second__estimator__n_estimators': [25,50,100],\n",
    "              'lr__C':[1,5,10]}\n",
    "\n",
    "gsLr = GridSearchCV(LrPipeline, param_grid, cv = cv)\n",
    "test(gsLr, Xtrain, Xtest, ytrain, ytest,'lr with grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the other pipelines at home\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch to FeatureUnion with a familiar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Familiar data\n",
    "xP = np.linspace(0.1,7,100)    \n",
    "\n",
    "#Adding uniform noise\n",
    "yP = np.log(xP) + np.sin(xP) + np.random.uniform(-0.5,0.5,len(xP))\n",
    "\n",
    "yGT = np.log(xP) + np.sin(xP)\n",
    "             \n",
    "XP = xP[:,np.newaxis]\n",
    "\n",
    "plt.plot(xP, yP,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "stepsPoly = [('poly', PolynomialFeatures(4)), \n",
    "             ('lr', LinearRegression()) ]\n",
    "\n",
    "pipePoly = Pipeline(stepsPoly)\n",
    "\n",
    "pipePoly.fit(XP,yP)\n",
    "yPredP = pipePoly.predict(XP)\n",
    "\n",
    "plt.plot(xP,yPredP,'r')\n",
    "plt.plot(xP,yGT,'k--')\n",
    "plt.scatter(xP,yP)\n",
    "plt.legend(['Polynomial Fit','Noiseless GT','Data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipePoly.named_steps['lr'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PolynomialFeatures(3)\n",
    "a.fit_transform(XP).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.transform([[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# What if we want to add more features?\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "featUn = FeatureUnion([('poly', PolynomialFeatures(3)),\n",
    "                       ('log', FunctionTransformer(np.log))])\n",
    "featUn.fit_transform(XP).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featUn.transform([[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepsfeatUn = [('featUn', featUn), \n",
    "               ('lr', LinearRegression()) ]\n",
    "\n",
    "pipefeatUn = Pipeline(stepsfeatUn)\n",
    "\n",
    "pipefeatUn.fit(XP,yP)\n",
    "yPredF = pipefeatUn.predict(XP)\n",
    "\n",
    "plt.plot(xP,yPredF,'r')\n",
    "plt.plot(xP,yPredP,'b')\n",
    "plt.plot(xP,yGT,'k--')\n",
    "plt.scatter(xP,yP)\n",
    "plt.legend(['Polynomial(3)+Log Fit','Polynomial(4) Fit','Noiseless GT','Data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featUn2 = FeatureUnion([('sin', FunctionTransformer(np.sin)),\n",
    "                        ('log', FunctionTransformer(np.log))])\n",
    "stepsfeatUn2 = [('featUn', featUn2), \n",
    "                ('lr', LinearRegression()) ]\n",
    "\n",
    "pipefeatUn2 = Pipeline(stepsfeatUn2)\n",
    "\n",
    "pipefeatUn2.fit(XP,yP)\n",
    "yPredF2 = pipefeatUn2.predict(XP)\n",
    "\n",
    "plt.plot(xP,yPredF2,'r')\n",
    "plt.plot(xP,yPredF,'g')\n",
    "plt.plot(xP,yPredP,'b')\n",
    "plt.plot(xP,yGT,'k--')\n",
    "plt.scatter(xP,yP)\n",
    "plt.legend(['Sine+Log Fit','Polynomial(3)+Log Fit','Polynomial(4) Fit','Noiseless GT','Data'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featUn2.transform([[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipefeatUn2.named_steps['lr'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipefeatUn.named_steps['lr'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't we combine multiple features and do feature selection with Lasso regression? Let's do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# We are now extending the required classes\n",
    "class GaussianRbfFeaturesPipeline(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, num_centers=10, width_constant=1.0):\n",
    "        #We set the centers and widths automatically\n",
    "        self.k = num_centers\n",
    "        self.h = width_constant\n",
    "    \n",
    "    @staticmethod\n",
    "    def _rbf(x,c,h):\n",
    "        # To handle multiple dimensions\n",
    "        return np.exp(-np.sum(((x-c)/h)**2, axis=1))\n",
    "    \n",
    "    # For the API\n",
    "    def fit(self,X,y=None):\n",
    "        self.centers_ = np.linspace(X.min(), X.max(), self.k)\n",
    "        self.widths_ = self.h*(self.centers_[1]-self.centers_[0])\n",
    "        return self\n",
    "    \n",
    "    # For the API\n",
    "    def transform(self,X):\n",
    "        return self._rbf(X[:, :, np.newaxis], self.centers_, self.widths_)\n",
    "    \n",
    "    # For the API\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X,y)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    # Additional methods for the hyperparameter search to work\n",
    "    def get_params(self, deep=True):\n",
    "        return {'num_centers':self.k,'width_constant':self.h}\n",
    "    \n",
    "    def set_params(self, **params):\n",
    "        self.k = params['num_centers']\n",
    "        self.h = params['width_constant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_feats = FeatureUnion([('poly', PolynomialFeatures(3)),('rbf',GaussianRbfFeaturesPipeline(8,3))])\n",
    "\n",
    "steps_many_feats = [('feats', many_feats), \n",
    "                    ('lr', LinearRegression())]\n",
    "\n",
    "pipe_mf = Pipeline(steps_many_feats)\n",
    "pipe_mf.fit(XP,yP)\n",
    "\n",
    "yPred_mf = pipe_mf.predict(XP)\n",
    "\n",
    "plt.plot(xP,yPred_mf,'r')\n",
    "plt.plot(xP,yGT,'k--')\n",
    "plt.scatter(xP,yP)\n",
    "plt.legend(['Poly+RBF','Noiseless GT','Data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe_mf.named_steps['lr'].coef_.shape) #3 degree + 1 bias + 8 rbf = 12\n",
    "\n",
    "print(pipe_mf.named_steps['lr'].coef_ )\n",
    "print(pipe_mf.named_steps['lr'].intercept_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "many_feats = FeatureUnion([('poly', PolynomialFeatures(3)),('rbf',GaussianRbfFeaturesPipeline(8,3))])\n",
    "steps_many_feats = [('feats', many_feats), \n",
    "                    ('lasso', Lasso(alpha=0.001,max_iter=50000))]\n",
    "\n",
    "pipe_mf = Pipeline(steps_many_feats)\n",
    "pipe_mf.fit(XP,yP)\n",
    "\n",
    "yPred_mf = pipe_mf.predict(XP)\n",
    "\n",
    "plt.plot(xP,yPred_mf,'r')\n",
    "plt.plot(xP,yGT,'k--')\n",
    "plt.scatter(xP,yP)\n",
    "plt.legend(['Poly+RBF','Noiseless GT','Data'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe_mf.named_steps['lasso'].coef_.shape) #3 degree + 1 bias + 8 rbf = 12\n",
    "\n",
    "print(pipe_mf.named_steps['lasso'].coef_ )\n",
    "print(pipe_mf.named_steps['lasso'].intercept_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can use feature union, pipelines and column transformers within each other! (e.g. parallel pipelines). This is called \"composing\" and these features will be incorporated within the compose submodule of scikit-learn in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to Titanic Dataset**\n",
    "\n",
    "So let's add the age feature now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**Lvl 3:**\n",
    "* pclass: leave as is or min max scaler\n",
    "* sibsp: log(1+x)\n",
    "* parch: drop\n",
    "* age: standard scaling\n",
    "* age to binary (<12) (yes this time!)\n",
    "* fare: log(1+x) then standard scaling\n",
    "\"\"\"\n",
    "\n",
    "def age_bin(x, thresh = 13):\n",
    "    X = np.zeros(x.shape)\n",
    "    for i in range(x.shape[1]):\n",
    "        X[:,i] = x[:,i]<thresh\n",
    "    return X\n",
    "\n",
    "interMediateAgeFeature = FunctionTransformer(age_bin, validate=True)\n",
    "\n",
    "interMediateFeatUn = FeatureUnion([('age_scaler',StandardScaler()),\n",
    "                                   ('age_bin',interMediateAgeFeature)])\n",
    "\n",
    "\n",
    "Log1pTransformer = FunctionTransformer(np.log1p, validate=True)\n",
    "sibsp = ('CountsLogT', Log1pTransformer, [6])\n",
    "parch = ('Drop','drop',[7])\n",
    "fare = ('Fare', Pipeline([('FareLT',Log1pTransformer), ('FareSc',StandardScaler())]), [8])\n",
    "\n",
    "# order after the below: age_scale, age_bin, sibsp, parch, age, fare, sex, embarked x 3 (1hot), pclass\n",
    "thirdLevelCT = ColumnTransformer([('Age Stuff', interMediateFeatUn, [5]),sibsp,parch,fare], remainder='passthrough')\n",
    "\n",
    "MainPipeLine2 = Pipeline ([('first', firstLevelCT),\n",
    "                          ('second', secondLevel),\n",
    "                          ('third', thirdLevelCT)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpPL = Pipeline ([('first', firstLevelCT), ('second', secondLevel)])\n",
    "\n",
    "xtmp = tmpPL.fit_transform(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtmp.shape)\n",
    "xtmp2 = interMediateCT.fit_transform(xtmp)\n",
    "print(xtmp2.shape)\n",
    "xtmp2[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LrPipeline2  = Pipeline([('main', MainPipeLine2), ('lr',  LogisticRegression())])\n",
    "\n",
    "test(LrPipeline2, Xtrain, Xtest, ytrain, ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

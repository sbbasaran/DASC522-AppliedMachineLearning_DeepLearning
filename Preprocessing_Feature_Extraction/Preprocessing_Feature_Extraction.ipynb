{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing, Feature Extraction and Pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# New submodules\n",
    "from sklearn import preprocessing\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Features of a dataset can be of different scales (e.g. salary vs age). This usually has a detrimental effect on many machine learning algorithms. As such, transforming features to have similar scales is widely used. Some methods for scaling:\n",
    "\n",
    "* **Z-Normalization:** Transform each dimension of the data to have 0 mean and 1 standard deviation.\n",
    "* **Min-Max Scaling:** Transform each dimension of the data so that it falls between two values. Usually this range is chosen as [0,1] or [-1,1].\n",
    "* **Max Absolute Value Scaling:** To transform the absolute value of each dimension of the data to be less than a certain value. Usually this value will be 1.\n",
    "* **Robust Scaling:**  *Outlier* values in the data can adversely affect scaling operations. A scaling approach using median and quartile ranges may be preferable when outliers are present. .\n",
    "* **Whitening:** Scaling and decorelating multiple dimensions to have 0-vector mean and unit covariance. We will see this when we go over Principal Component Analysis. ([Wikipedia entry](https://en.wikipedia.org/wiki/Whitening_transformation))\n",
    "\n",
    "**Note**: Care should be taken if the data containing many 0s (sparse data) is to be preprocessed. It is easy to deteriorate their sparse structure, leading to many issues. The methods that change the *center* of the data is the main culprit (e.g. z-normalization). Methods that keep the sparse nature intact should be chosen (e.g. max absolute value scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.],\n",
    "              [ 1.,  0.,  1.]])\n",
    "print(\"Original:\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-Normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xzn = preprocessing.scale(X)\n",
    "print(\"Z-Normalized:\")\n",
    "print(Xzn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before: Mean and Standard Deviation\")\n",
    "print(X.mean(axis = 0), X.std(axis = 0))\n",
    "print()\n",
    "print(\"After: Mean and Standard Deviation\")\n",
    "print(Xzn.mean(axis = 0), Xzn.std(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Keeping the scaling information:\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Calculate the scaling values:\n",
    "scaler.fit(X) \n",
    "print(\"Scaling Class:\")\n",
    "print(scaler)\n",
    "print()\n",
    "print(\"Scaling Values:\")\n",
    "print(scaler.mean_, scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Z-Normalized:\")\n",
    "\n",
    "# Scale the incoming data\n",
    "print(scaler.transform(X))\n",
    "\n",
    "# TRANSFORMER API!!!\n",
    "\n",
    "# fit(train) + transform(train) + transform(test)\n",
    "# fit_transform(train) + transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Calculate the scaling values and scale the original data in a single line\n",
    "Xmm = min_max_scaler.fit_transform(X)\n",
    "print(\"Original\")\n",
    "print(X)\n",
    "print()\n",
    "print(\"Range [0, 1]:\")\n",
    "print(Xmm)\n",
    "print()\n",
    "\n",
    "print(\"Scaling Values:\")\n",
    "print(min_max_scaler.scale_, min_max_scaler.min_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler.inverse_transform(Xmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([[1.5, -1., 2.5]])\n",
    "print(\"When new data comes: \") \n",
    "print(Y)\n",
    "print(min_max_scaler.transform(Y))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original\")\n",
    "print(X)\n",
    "print()\n",
    "print(\"Range [-1, 1] \")\n",
    "min_max_scaler_range = preprocessing.MinMaxScaler(feature_range = [-1, 1])\n",
    "\n",
    "print(min_max_scaler_range.fit_transform(X)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute Value Scaling (Better for Sparse Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "Xma = max_abs_scaler.fit_transform(X)\n",
    "print(\"Original\")\n",
    "print(X)\n",
    "print()\n",
    "print(\"Absolute Value to 1:\")\n",
    "print(Xma)\n",
    "print()\n",
    "print(\"Scaling:\")\n",
    "print(max_abs_scaler.scale_)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With different data\")\n",
    "Y = np.array([[ -3., -1.,  4.]])\n",
    "print(Y)\n",
    "print(max_abs_scaler.transform(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling with Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random data generation\n",
    "Xo = np.random.standard_t(2, (50,2))\n",
    "plt.boxplot(Xo)\n",
    "plt.title(\"Data \")\n",
    "down, up = plt.ylim()\n",
    "plt.show()\n",
    "\n",
    "z_scaler = preprocessing.StandardScaler()\n",
    "Xz = z_scaler.fit_transform(Xo)\n",
    "plt.boxplot(Xz)\n",
    "plt.title(\"Z-Normalized\")\n",
    "plt.ylim(down, up)\n",
    "plt.show()\n",
    "\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "Xrs = robust_scaler.fit_transform(Xo)\n",
    "plt.boxplot(Xrs)\n",
    "plt.title(\"Robust\")\n",
    "plt.ylim(down, up)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "bc_dataset = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(bc_dataset[\"data\"][:,-7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot(preprocessing.MinMaxScaler().fit_transform(bc_dataset[\"data\"][:,-7].reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(preprocessing.RobustScaler().fit_transform(bc_dataset[\"data\"][:,-7].reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data is inherently multi-dimensional (e.g. orientation of a rigid body) and thus it is not appropriate to scale them by-themselves. We need to scale them together. One such method is data whitening. Another method is unit normalization where we normalize the data points (or a feature subset of the data points)  to have unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Original:\")\n",
    "print(X)\n",
    "print()\n",
    "\n",
    "Xn = preprocessing.normalize(X, norm = \"l2\")\n",
    "print(\"Normalize to Unit Vectors:\")\n",
    "print(Xn)\n",
    "print()\n",
    "\n",
    "# when you need a class (e.g. to put in a pipeline)\n",
    "normalizer = preprocessing.Normalizer()\n",
    "print(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't do anything but it is there so that since the Normalizer class \n",
    "# fully implements the Transformer API (useful for using it in a pipeline)\n",
    "normalizer.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will talk about Data Whitening when we cover Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data Encoding\n",
    "\n",
    "Many learning algorithms expect numerical values (vector, matrix, etc.) as input. Therefore, it is necessary to encode categorical data this way. For example, consider customer data: `[gender, city, occupation]`. Let there be `[female, male]` for gender, `[ankara, istanbul, izmir]` for city, and `[private, public, freelance, retired]` for occupation.\n",
    "\n",
    "How can we convert categories to numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder()\n",
    "\n",
    "M = [[\"male\", \"ankara\", \"public\"], \n",
    "     [\"female\", \"istanbul\", \"private\"],\n",
    "     [\"female\", \"izmir\", \"retired\"],\n",
    "     [\"male\", \"istanbul\", \"freelance\"]]\n",
    "\n",
    "# Getting the categories from the data\n",
    "enc.fit(M)\n",
    "print(\"Encoder:\")\n",
    "print(enc)\n",
    "print(\"Encoding:\")\n",
    "# 2 (gender) + 3 (city) + 4 (occupation) = 9 dimensional\n",
    "print(enc.transform(M).toarray())\n",
    "\n",
    "# New data\n",
    "print(enc.transform([[\"female\", \"istanbul\", \"freelance\"],\n",
    "                     [\"male\", \"ankara\", \"retired\"]]).toarray())\n",
    "\n",
    "print(\"Categories:\")\n",
    "print(enc.categories_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It is a better idea to map binary categories to a single dimension (0-1) instead of a 2 dimensional 1-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.transform([[\"female\", \"izmir\", \"engineer\"]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving categories by hand\n",
    "gender = [\"female\", \"male\"] \n",
    "city = [\"ankara\", \"istanbul\", \"izmir\", \"kocaeli\"]\n",
    "occupation = [\"private\", \"public\", \"freelance\", \"retired\"]\n",
    "\n",
    "enc2 = preprocessing.OneHotEncoder(categories = [gender, city, occupation])\n",
    "print(\"Encoder:\")\n",
    "print(enc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enc2.transform([[\"kadın\", \"istanbul\", \"freelance\"],\n",
    "                      [\"erkek\", \"kocaeli\", \"retired\"]]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still need to call the fit\n",
    "enc2.fit(M)\n",
    "print(\"Encoding:\")\n",
    "print(enc2.transform([[\"female\", \"istanbul\", \"freelance\"],\n",
    "                      [\"male\", \"kocaeli\", \"retired\"]]).toarray())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is a chance of getting an unexpected category (e.g. missing data)\n",
    "enc3 = preprocessing.OneHotEncoder(handle_unknown = \"ignore\")\n",
    "enc3.fit(M) \n",
    "print(\"Encoder:\")\n",
    "print(enc3)\n",
    "print(\"Unknown category is mapped to a 0-vector of appropriate size:\")\n",
    "print(enc3.transform([[\"female\", \"bursa\", \"public\"]]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes data comes in a dictionary format (e.g. JSON). There is the `DictVectorizer` class for these cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dictData = [\n",
    "    {\"price\": 1200000, \"room\": 4, \"neighborhood\": \"Maslak\", \"purpose\": \"business\"},\n",
    "    {\"price\": 1400000, \"room\": 3, \"neighborhood\": \"Etiler\", \"purpose\": \"house\"},\n",
    "    {\"price\":  500000, \"room\": 3, \"neighborhood\": \"Tuzla\",  \"purpose\": \"house\"},\n",
    "    {\"price\":  900000, \"room\": 2, \"purpose\": \"business\", \"neighborhood\": \"Etiler\"}]\n",
    "vec = DictVectorizer(sparse = False, dtype = int)\n",
    "print(vec.fit_transform(dictData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.feature_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It only converts strings to one-hot vectors. If the categorical data is given with integers, we need to use the `OneHotEncoder` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Category Encoders**\n",
    "\n",
    "When there are a lot of categories and/or imbalanced category distribution, one-hot encoding does not work very well. Some domains also like to encode continuous variables. For a wide variety of categorical encoders (including the famous weight of evidence in banking): https://contrib.scikit-learn.org/category_encoders/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "encoder = ce.WOEEncoder(cols = [...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Missing Data: Imputation\n",
    "\n",
    "Some portions of real-world data is often missing. There are different ways to deal with this. The simplest and most commonly used methods include using the mean, median, mode, or a constant value to fill the missing data. \n",
    "\n",
    "Ignoring data points with missing features or using machine learning to fill them are also common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "X = np.array([[1, 2], \n",
    "              [np.nan, 3], \n",
    "              [7, 6], \n",
    "              [5, 3], \n",
    "              [4, 4]])\n",
    "print(\"Initial Data\")\n",
    "print(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "imp = SimpleImputer(missing_values = np.nan, strategy = \"mean\")\n",
    "imp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe = np.array([[np.nan, 2], \n",
    "               [6, np.nan], \n",
    "               [7, 6]])\n",
    "print(\"New Data\")\n",
    "print(Xe)\n",
    "\n",
    "print(\"Mean:\")\n",
    "print(imp.transform(Xe))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median\n",
    "imp2 = SimpleImputer(missing_values = -999, strategy = \"median\")\n",
    "X = np.array([[1,    2], \n",
    "              [-999, 3], \n",
    "              [7,    6]])\n",
    "\n",
    "print(\"Initial Data\")\n",
    "print(X)\n",
    "print()\n",
    "\n",
    "Xe = np.array([[-999,   2], \n",
    "               [6,   -999], \n",
    "               [7,      6]])\n",
    "print(\"New Data\")\n",
    "print(Xe)\n",
    "print()\n",
    "\n",
    "imp2.fit(X) \n",
    "print(\"Median:\")\n",
    "print(imp2.transform(Xe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = imp2.fit_transform(X)\n",
    "print(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[\"a\", \"y\"],\n",
    "                   [np.nan, \"x\"],\n",
    "                   [\"a\", np.nan],\n",
    "                   [\"b\", \"y\"]], dtype = \"category\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most common based on available categories in the Dataframe:\")\n",
    "imp3 = SimpleImputer(strategy = \"most_frequent\")\n",
    "tmp = imp3.fit_transform(df)\n",
    "print(tmp)\n",
    "print(type(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pandas Dataframe\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "print(\"Fill with Constant:\")\n",
    "imp4 = SimpleImputer(strategy = \"constant\", fill_value = \"c\")\n",
    "print(imp4.fit_transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method = \"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on your scikit-learn version you may or may not need the below line\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Chose the predictor: \n",
    "ite_imp = IterativeImputer(estimator = LinearRegression())\n",
    "X = np.array([[7, 2, 3], \n",
    "              [4, np.nan, 6], \n",
    "              [10, 5, 9]])\n",
    "print(X)\n",
    "print()\n",
    "ite_imp.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe = np.array([[np.nan, 2, 3], \n",
    "               [4, np.nan, 6], \n",
    "               [10, np.nan, 9]])\n",
    "print(Xe)\n",
    "print()\n",
    "ite_imp.transform(Xe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe2 =np.array([[np.nan, 4, 5], \n",
    "               [2, np.nan, 8], \n",
    "               [12, np.nan, 6]])\n",
    "\n",
    "ite_imp.transform(Xe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ite_imp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features\n",
    "\n",
    "Some problems benefit from non-linearity. One way to get this is to create non-linear features from data. Linear estimators can be turned into non-linear ones this way. One of the most common ways of doing this is to add polynomials of the inputs. We can do this with the `PolynomialFeatures` class. \n",
    "\n",
    "Another common method is to use RBF functions but we will cover it in the regression part of the lecture. There are also others. Nowadays however, we rely more on the non-linearity of the methods instead of extracting non-linear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "Xp = np.arange(6).reshape(3, 2)\n",
    "print(\"Original:\")\n",
    "print(Xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2) #exhaustive to 2nd degree\n",
    "print(\"With polynomial features added: 1 x1 x2 (x1)^2 x1*x2 (x2)^2\")\n",
    "print(poly.fit_transform(Xp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly2 = PolynomialFeatures(degree = 2, \n",
    "                           interaction_only = True, \n",
    "                           include_bias = False)\n",
    "print(\"Sadece etkileşim Polinom Öznitelikleri Eklenmiş: x1 x2 x1x2\")\n",
    "print(poly2.fit_transform(Xp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions for Preprocessing\n",
    "\n",
    "Sometimes we want to use custom functions for preprocessing (e.g. taking the logarithm to make a feature more symmetric). There are multiple ways of doing this. However, the below one is recommended so that we can include them easily in Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "transformer = FunctionTransformer(np.log1p, validate = True)\n",
    "Xt = np.array([[0, 1], [2, 3]])\n",
    "print(Xt)\n",
    "print()\n",
    "print(np.log1p(Xt))\n",
    "print()\n",
    "print(transformer.transform(Xt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Features\n",
    "\n",
    "We need to convert text data into numerical values so that we can use it in ML algorithms. There are many approaches towards this end. `scikit-learn` provides implementations for two older but still useful methods. The first one is the `bag-of-words` model which uses word counts directly and the other one is the `tf-idf` model which trade-ofss the counts of the words with their commonality (if a word is common everywhere, it may not be important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "example = [\"It is hot today\", \n",
    "           \"The hot hot and sour soup\", \n",
    "           \"Air, water, road and electricity\", \n",
    "           \"On the road today\"]\n",
    "\n",
    "print(\"Count Based\")\n",
    "vec = CountVectorizer()\n",
    "Xbow = vec.fit_transform(example)\n",
    "display(pd.DataFrame(Xbow.toarray(), columns = vec.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = [\"Hot air soup\"]\n",
    "display(pd.DataFrame(vec.transform(test_example).toarray(), columns = vec.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example2 = [\"Hot air balloon\"]\n",
    "display(pd.DataFrame(vec.transform(test_example2).toarray(), columns = vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1sw = CountVectorizer(stop_words=['it','and'])\n",
    "Xbow1sw = vec1sw.fit_transform(example)\n",
    "display(pd.DataFrame(Xbow1sw.toarray(), columns = vec1sw.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TF-IDF\")\n",
    "vec2 = TfidfVectorizer()\n",
    "Xtfidf = vec2.fit_transform(example)\n",
    "display(pd.DataFrame(Xtfidf.toarray(), columns = vec.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.linalg.norm(Xtfidf[i,:].toarray()) for i in range(Xtfidf.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use these features in a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining Pre-Processing, Feature Extraction and Prediction Steps\n",
    "\n",
    "In most machine learning applications, we perform several \"operations\" to the data before inputting them to a learning algorithm.\n",
    "\n",
    "For example:  \n",
    "1. Fill the missing values with means \n",
    "2. Scale the data to the [0,1] range\n",
    "3. Add second degree polynomial features\n",
    "4. Fit a linear model to the data\n",
    "\n",
    "The data enters the learning process after undergoing some transformation. We have seen the Transformer API before which are for the transformation algorithms. This API defines the `fit()` and `transform()` functions along with the `fit_transform()` function, that does both in a single step.\n",
    "\n",
    "The `scikit-learn` module has `pipeline` functionality to abstract and simplify this process. This abstraction represents the input of data passing through multiple transformers into an estimator in its final form. Let's see how it is used:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Input\n",
    "Xpp = np.array([[ np.nan, 0,   3  ],\n",
    "                [ 3,      7,   9  ],\n",
    "                [ 3,      5,   2  ],\n",
    "                [ 4, np.nan,   6  ],\n",
    "                [ 8,      8,   1  ]])\n",
    "\n",
    "# Target\n",
    "ypp = np.array([14.2, 15.9, -1.01,  7.93, -5.2])\n",
    "\n",
    "#Fill the missing values with means\n",
    "simple_imp = SimpleImputer(strategy=\"mean\")\n",
    "Ximp = simple_imp.fit_transform(Xpp)\n",
    "\n",
    "#Scale the data to the [0,1] range\n",
    "mm_scaler = MinMaxScaler()\n",
    "Xsca = mm_scaler.fit_transform(Ximp)\n",
    "\n",
    "#Add second degree polynomial features\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "Xfeats = pf.fit_transform(Xsca)\n",
    "\n",
    "#Fit a linear model to the data\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(Xfeats, ypp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training\")\n",
    "print(model.predict(Xfeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xfeats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XppTest = np.array([[     2,      6, np.nan],\n",
    "                    [np.nan, np.nan, np.nan]])\n",
    "\n",
    "XTimp = simple_imp.transform(XppTest)\n",
    "XTsca = mm_scaler.transform(XTimp)\n",
    "XTfeats = pf.transform(XTsca)\n",
    "\n",
    "print(\"Testing\")\n",
    "print(model.predict(XTfeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "                     # 1. Fill the missing values with means \n",
    "pipe = make_pipeline(SimpleImputer(missing_values = np.nan, strategy = \"mean\"),\n",
    "                     # 2. Scale the data to the [0,1] range                     \n",
    "                     preprocessing.MinMaxScaler(),\n",
    "                     # 3. Add second degree polynomial features\n",
    "                     PolynomialFeatures(degree = 2),\n",
    "                     # 4. Fit a linear model to the data\n",
    "                     LinearRegression())\n",
    "\n",
    "print(type(pipe))\n",
    "print(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also label the steps (useful for later)\n",
    "from sklearn.pipeline import Pipeline\n",
    "steps = [(\"impute\", SimpleImputer(missing_values = np.nan, strategy = \"mean\")), \n",
    "         (\"scale\", preprocessing.MinMaxScaler()),\n",
    "         (\"poly\", PolynomialFeatures(degree = 2)),\n",
    "         (\"learn\", LinearRegression())]\n",
    "\n",
    "pipe2 = Pipeline(steps)\n",
    "print(pipe2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting Pipeline object implements `fit` and `predict` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting\n",
    "pipe.fit(Xpp, ypp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Targets:\")\n",
    "print(ypp)\n",
    "print()\n",
    "print(\"Training Predictions:\")\n",
    "print(pipe.predict(Xpp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Prediction:\")\n",
    "print(pipe.predict(XppTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepsNoLearn = [(\"impute\", SimpleImputer(missing_values = np.nan, strategy = \"mean\")), \n",
    "                (\"scale\", preprocessing.MinMaxScaler()),\n",
    "                (\"poly\", PolynomialFeatures(degree = 2))]\n",
    "\n",
    "pipeNL = Pipeline(stepsNoLearn)\n",
    "Xfeats2 = pipeNL.fit_transform(Xpp)\n",
    "Xfeats2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the individual steps:\n",
    "tmp = pipe.named_steps['polynomialfeatures']\n",
    "tmp.powers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create pipelines with any number of transformers and optionally a single predictor at the end.\n",
    "\n",
    "In effect, pipelines perform back to back `fit_transform` operations and feed the outputs of prior transformers as the inputs of the next ones. They are mostly multi-input and multi-output and apply the same step to all the measurements. (some only accept 1D data and output 1D data, you can custom write transformers that apply different operations to different dimensions) \n",
    "\n",
    "**Is this good enough?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "Xpp2 = np.array([[ np.nan, 'a',   3  ],\n",
    "                 [ 3,      'b',   9  ],\n",
    "                 [ 3,      'a',   2  ],\n",
    "                 [ 4, np.nan,   6  ],\n",
    "                 [ 8,      'c',   1  ]])\n",
    "\n",
    "# Target\n",
    "ypp2 = np.array([14.2, 15.9, -1.01,  7.93, -5.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2.fit(Xpp2,ypp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not Enough:**  \n",
    "* What if we have different data types? The transformers in question are designed mostly for a single type of data (e.g. scalers vs categoric variables). \n",
    "* What if we want to apply different pre-processing steps to same type of data? (e.g. log transform to count variables and keeping the others as is before scaling)\n",
    "* What if we want to extract multiple types of features in parallel? (e.g. polynomial and radial basis for regression)?\n",
    "\n",
    "For these, we are going to use the `ColumnTransformer` and `FeatureUnion` classes. Before those, let's do an exercise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "It is useful to reduce the dimensionality of our data in various situations such as we do not have enough points for the size of the data, we think that our data comes from a low-dimensional space or we want to interpret/visualize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go over the `digits` dataset once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X = digits[\"data\"]\n",
    "y = digits[\"target\"]\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize = (8, 8),\n",
    "                         subplot_kw = {\"xticks\": [], \"yticks\": []},\n",
    "                         gridspec_kw = dict(hspace = 0.1, wspace = 0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap = \"binary\", interpolation = \"nearest\")\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform = ax.transAxes, color = \"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is between 0 and 16, let's scale it between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Let's start with PCA, which is a multi-purpose and fundamental algorithm. It is perhaps the most commonly used dimensionality reduction approach. PCA separates data into uncorrelated (0 covariance) components. It is readily available in sckit-learn. It has the `fit`, `transform` and `fit_transform` functions that we are familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 64)\n",
    "pca.fit(X)\n",
    "print(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "explained_variance_cs = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.plot(explained_variance_cs)\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of components for at least 90% variance explanation:', \n",
    "      np.where(explained_variance_cs>0.90)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 8, figsize = (8, 8),\n",
    "                         subplot_kw = {\"xticks\": [], \"yticks\": []},\n",
    "                         gridspec_kw = dict(hspace = 0.1, wspace = 0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(8, 8), cmap = \"binary\", interpolation = \"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca40 = PCA(n_components = 40)\n",
    "X_transformed = pca40.inverse_transform(pca40.fit_transform(X))\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize = (8, 8),\n",
    "                         subplot_kw = {\"xticks\": [], \"yticks\": []},\n",
    "                         gridspec_kw = dict(hspace = 0.1, wspace = 0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_transformed[i,:].reshape(8, 8), cmap = \"binary\", interpolation = \"nearest\")\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]),\n",
    "            transform = ax.transAxes, color = \"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's project our multi-dimensional data (64 for the digits dataset) to 2 dimensions and visualize the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components = 2)\n",
    "Xt = pca2.fit_transform(X)\n",
    "print(pca2)\n",
    "\n",
    "print(Xt.shape)\n",
    "\n",
    "print(np.sum(pca2.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDigitProjections(Xin, y, title = None):\n",
    "    plt.figure(figsize = (16,10))\n",
    "    for c in range(10):\n",
    "        indexes = np.where(y == c)[0]\n",
    "        x = Xin[indexes]\n",
    "        plt.scatter(x[:, 0], x[:, 1], label = c)\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.legend()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotDigitProjections(Xt, y, \"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "\n",
    "PCA is an unsupervised method. It does not take the labels into accoutt. LDA uses label information for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(n_components = 2)\n",
    "Xtlda = lda.fit_transform(X, y)\n",
    "print(np.sum(lda.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDigitProjections(Xtlda, y, \"LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained variance for the LDA method is higher than the PCA method!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PCA for Data Whitening\n",
    "\n",
    "Some problems require each principal component to be on the same scale. Converting data to its principal components and scaling each component to have unit variance is called data whitening. Data whitening ensures that all dimensions have the same scale relative to each other. This can be good (naturally scaled data) or bad (increasing importance of noisy components). In addition, some methods assume unit covariance (e.g., LDA assumes that every class has the same covariance matrix)onto separating the principal components of the data.\n",
    "\n",
    "It's easy to use the `PCA` class in `scikit-learn` for data bleaching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(whiten = True)\n",
    "Xwhitened = pca.fit_transform(X)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (12, 8))\n",
    "axes[0].matshow(np.cov(X.T))\n",
    "axes[0].set_title(\"Covariance of the Original Data\", {\"fontsize\": 18})\n",
    "axes[1].matshow(np.cov(Xwhitened.T))\n",
    "axes[1].set_title(\"Covariance of the Whitened Data\", {\"fontsize\": 18})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Nonlinear Methods\n",
    "\n",
    "PCA ve LDA are linear methods. However, the data might have a non-linear subspace or manifold. We need nonlinear methods for this type of data.\n",
    "\n",
    "We will only demonstrate the methods and will not go into detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Dimensional Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "#Takes a while to run\n",
    "mds = MDS(n_components = 2)\n",
    "Xtmds = mds.fit_transform(X)\n",
    "plotDigitProjections(Xtmds, y, \"MDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "Xtiso = Isomap(n_components = 2).fit_transform(X)\n",
    "plotDigitProjections(Xtiso, y, \"Isomap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "\n",
    "This is one of the most commonly used method for data visualization by mapping it to 2D or 3D.\n",
    "\n",
    "However, it cannot perform out-of-sample dimensionality reduction (hence only used for visualizing data). As a result, you should not use it for learning (there are exceptions to this rule, e.g., for feature extraction from a known and finite set of data)\n",
    "\n",
    "There is a useful [FAQ](https://lvdmaaten.github.io/tsne/) from the algorithm developer.\n",
    "\n",
    "See also https://projector.tensorflow.org/ for a variety of methods for your visualization needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "Xtsne = TSNE(n_components = 2).fit_transform(X)\n",
    "plotDigitProjections(Xtsne, y, \"t-SNE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDigitProjections3D(Xin, y, title = None):\n",
    "    ax = plt.axes(projection='3d')\n",
    "    for c in range(10):\n",
    "        indexes = np.where(y == c)[0]\n",
    "        x = Xin[indexes]\n",
    "        ax.scatter(x[:, 0], x[:, 1], x[:, 2], label = c)\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.set_zlabel('Component 3')\n",
    "\n",
    "    plt.legend()\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D version\n",
    "%matplotlib notebook \n",
    "\n",
    "Xtsne3D = TSNE(n_components = 3).fit_transform(X)\n",
    "\n",
    "plotDigitProjections3D(Xtsne3D,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use the digits data-set along with a dimensionality reduction method and perform classification using Pipelines. The baseline is provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, np.max(X), np.min(X), X.dtype, y.shape)\n",
    "print(np.unique(y))\n",
    "sns.countplot(x = y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, stratify = y, test_size=0.4, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logRegClassifier = LogisticRegression(C = 1.0, max_iter = 10000)\n",
    "\n",
    "logRegClassifier.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrainPred = logRegClassifier.predict(Xtrain)\n",
    "print(\"Training set performance:\", accuracy_score(ytrain, ytrainPred))\n",
    "ypred = logRegClassifier.predict(Xtest)\n",
    "print(\"Test set performance:\", accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline: dimensionality reduction + classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Home Exercise:** Try it with other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example (Eigenfaces):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person = 60)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(150, svd_solver = \"randomized\")\n",
    "pca.fit(faces.data)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, 8, figsize = (9, 4),\n",
    "                         subplot_kw = {\"xticks\": [], \"yticks\": []},\n",
    "                         gridspec_kw = dict(hspace = 0.1, wspace = 0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(62, 47), cmap = \"bone\")\n",
    "\n",
    "components = pca.transform(faces.data)\n",
    "projected = pca.inverse_transform(components)\n",
    "\n",
    "fig, ax = plt.subplots(2, 10, figsize = (10, 2.5),\n",
    "                       subplot_kw = {\"xticks\": [], \"yticks\": []},\n",
    "                       gridspec_kw = dict(hspace = 0.1, wspace = 0.1))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap = \"binary_r\")\n",
    "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap = \"binary_r\")\n",
    "\n",
    "ax[0, 0].set_ylabel(\"full-dim\\ninput\")\n",
    "ax[1, 0].set_ylabel(\"150-dim\\nreconstruction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "u = umap.UMAP(n_components=2)\n",
    "Xumap = u.fit_transform(X)\n",
    "\n",
    "plotDigitProjections(Xumap, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDigitProjections(Xtsne, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.transform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
